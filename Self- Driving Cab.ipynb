{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project of RL CISC856- Mina Rahmanian.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiQTf_pdIB9X"
      },
      "source": [
        "# **Project of Reinfocement Learning**\n",
        "\n",
        "\n",
        "\n",
        "## Self-Driving cab\n",
        "\n",
        "### Mina Rahmanian Shahri (20137470)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzV5jw9SYLVj"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twqZ2hD6udxy"
      },
      "source": [
        " \n",
        "# ----------------------------   Q-learning   ----------------------------------\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1iQgLO47_3Nfzod55ttzWkDA16LrWKCMN\" width=\"650\" height=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp0wXPI3GFAL"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.render()\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))\n",
        "print('Blue colored letter denotes the pickup location and purple colored letter denotes the drop location.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJD4VYrtGJLD"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "env.s=state\n",
        "env.render()\n",
        "env.reset()\n",
        "env.step(5) # any number we can use instead of 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxwOn7aYc0dK"
      },
      "source": [
        "# Environment of  Self-Driving cab\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RAr92SAZ7Ne0clbfkTuZP3jHGQANnGgw\" width=\"350\" height=\"300\">\n",
        "\n",
        "*   Four different locations (R, G, Y, B)\n",
        "*   Taxi environment has 5(row) × 5(col) × 5 (4+1= five passenger location) × 4(destionation) = 500 total possible states.\n",
        "*    Six possible actions:   South,  North, West, East, Pickup, Dropoff\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWn5GHLnLyA-"
      },
      "source": [
        "## Colored tiles have the following meanings For a particular iteration:\n",
        "\n",
        "Yellow: The starting position of the taxi./ When the passenger is not in the taxi. & Once the taxi drops the passenger off, it's colour changes back to yellow.\n",
        "Blue: The position of the passenger./ The pickup spot.\\\n",
        "Purple: The destination of the passenger./ The drop off spot.\\\n",
        "Green: The position of the taxi with the passenger./ The passenger has been picked up by the taxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoBXEj2RGOI5"
      },
      "source": [
        "env.P[state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uafro7KE61-"
      },
      "source": [
        "*   $\\Large \\alpha$ (alpha) is the learning rate (0<$\\Large \\alpha$<=1) \n",
        "*   $\\Large \\gamma$ (gamma) is the discount factor ($0 \\leq \\gamma \\leq 1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
        "\n",
        "\n",
        "\n",
        "![q-value euation](https://render.githubusercontent.com/render/math?math=%5CLarge%20Q%28%7B%5Csmall%20state%7D%2C%20%7B%5Csmall%20action%7D%29%20%5Cleftarrow%20%281%20-%20%5Calpha%29%20Q%28%7B%5Csmall%20state%7D%2C%20%7B%5Csmall%20action%7D%29%20%2B%20%5Calpha%20%5CBig%28%7B%5Csmall%20reward%7D%20%2B%20%5Cgamma%20%5Cmax_%7Ba%7D%20Q%28%7B%5Csmall%20next%20%5C%20state%7D%2C%20%7B%5Csmall%20all%20%5C%20actions%7D%29%5CBig%29&mode=display)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2No_AtYGR17"
      },
      "source": [
        "\n",
        "def brute_force(episodes):\n",
        "    print(\"Running Brute Force....\")\n",
        "    performance_matrix=[]  \n",
        "    frames=[]\n",
        "    for episode in range(episodes):\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Progress: {(episode/episodes)*100}%\")\n",
        "        state=env.reset()\n",
        "        epochs, penalties, reward, = 0, 0, 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, done, info = env.step(action)\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "            \n",
        "            frames.append({\n",
        "                'episode': episode,\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward})\n",
        "            \n",
        "            epochs += 1\n",
        "        performance_matrix.append([epochs,penalties])\n",
        "\n",
        "    return performance_matrix,frames\n",
        "\n",
        "\n",
        "\n",
        "def q_learning(q_table,episodes):\n",
        "    print(\"Running Q-Learning...\")\n",
        "    frames = [] \n",
        "    performance_matrix=[]\n",
        "    for episode in range(episodes):\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Progress: {(episode/episodes)*100}%\")\n",
        "        state = env.reset()\n",
        "        epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "        done = False\n",
        "    \n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])\n",
        "            state, reward, done, info = env.step(action)\n",
        "\n",
        "            frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'episode':episode,\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward})\n",
        "\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "            epochs += 1\n",
        "\n",
        "        performance_matrix.append([epochs,penalties])\n",
        "\n",
        "    return performance_matrix,frames\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_frames(frames,sleep_time=0.1,**kwargs):\n",
        "    if 'episode' in kwargs:\n",
        "        frames=list(filter(lambda render: render['episode'] == kwargs['episode'], frames))\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Epsiode: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(sleep_time)\n",
        "\n",
        "\n",
        "\n",
        "def plot_performance(title,performance_matrix,**kwargs):\n",
        "\n",
        "    epochs=[]\n",
        "    penalties=[]\n",
        "    explore=[]\n",
        "    exploit=[]\n",
        "\n",
        "    for i,episode_performance in enumerate(performance_matrix):\n",
        "        if 'interval' in kwargs:\n",
        "            if i%kwargs['interval']==0:\n",
        "                epochs.append(episode_performance[0])\n",
        "                penalties.append(episode_performance[1])\n",
        "        else:\n",
        "            epochs.append(episode_performance[0])\n",
        "            penalties.append(episode_performance[1])\n",
        "    \n",
        "    df=pd.DataFrame({'episodes': range(0,len(epochs)), 'epochs': epochs, 'penalties': penalties })\n",
        "    plt.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "    #plt.ylim(top=toplimit)\n",
        "\n",
        "    if 'ylimit' in kwargs:\n",
        "        plt.ylim(kwargs['ylimit'])\n",
        "    if 'xlimit' in kwargs:\n",
        "        plt.xlim(kwargs['xlimit'])\n",
        "    if 'xlabel' in kwargs:\n",
        "        plt.xlabel(kwargs['xlabel'])\n",
        "    if 'ylabel' in kwargs:\n",
        "        plt.ylabel(kwargs['ylabel'])\n",
        "\n",
        "    plt.plot( 'episodes', 'epochs', data=df, marker='o', markerfacecolor='navy', markersize=1, color='mediumblue', linewidth=4)\n",
        "    plt.plot( 'episodes', 'penalties', data=df, marker='o', markerfacecolor='darkred', markersize=1, color='darkred', linewidth=4)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    if 'save' in kwargs:\n",
        "        plt.savefig(f\"graphs/{kwargs['save']}\")\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def q_learning_train(env,alpha=0.3,gamma=0.85,epsilon=0.5,iterations=1001,**kwargs):\n",
        "\n",
        "    print(f\"Running q_learning with alpha={alpha}, gamma={gamma}, epsilon={epsilon}, and {iterations-1} iterations\")\n",
        "    dh = display('',display_id=True)\n",
        "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "    #initialize the q-table as a 500 X 6 matrix of zeros as there are 500 states (5*5*5*4) and 6 actions\n",
        "    all_epochs = []\n",
        "    all_penalties = []\n",
        "    q_learning_performance_matrix = []\n",
        "    frames=[]\n",
        "\n",
        "    for i in range(1, iterations):\n",
        "        state = env.reset()\n",
        "\n",
        "        epochs, penalties, reward = 0, 0, 0\n",
        "        done = False\n",
        "    \n",
        "        while not done:\n",
        "\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = np.argmax(q_table[state]) # Exploit learned values\n",
        "            else:\n",
        "                action = env.action_space.sample() # Explore action space\n",
        "\n",
        "            next_state, reward, done, info = env.step(action) \n",
        "            frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'episode':i,\n",
        "            'state': state,\n",
        "            'action': action,  \n",
        "            'reward': reward})\n",
        "        \n",
        "            old_value = q_table[state, action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "        \n",
        "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "            q_table[state, action] = new_value\n",
        "\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "            state = next_state\n",
        "            epochs += 1\n",
        "        q_learning_performance_matrix.append([epochs,penalties])\n",
        "        if i % 100 == 0:\n",
        "            #clear_output(wait=True)\n",
        "            dh.update(f\"Episode: {i}\")\n",
        "\n",
        "    print(\"Training finished.\\n\")\n",
        "\n",
        "    return iterations,frames,q_table,q_learning_performance_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_performance_df(performance_matrix):\n",
        "    total=len(performance_matrix)\n",
        "    sum_epochs,sum_penalties=0,0\n",
        "    for metric in performance_matrix:\n",
        "        sum_epochs+=metric[0]\n",
        "        sum_penalties+=metric[1]\n",
        "        \n",
        "    return [sum_epochs/total,sum_penalties/total]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6_ClTyhNBMt"
      },
      "source": [
        "# Running through of the environment you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVBiNLyVVqk7"
      },
      "source": [
        "#q_learning_train(env,0.1,0.6,0.5,1001)\n",
        "\n",
        "iterations,frames1,q_table1,q_learning_performance_matrix1 = q_learning_train(env,alpha=0.3,gamma=0.85)\n",
        "performance_matrix,frames1 = q_learning(q_table1,1001)\n",
        "print_frames(frames1,0.1)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjCDRQ8cNTqx"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvHhH-EhGaaZ"
      },
      "source": [
        "iterations,q_learning_frame_greedy,q_table_greedy,q_learning_performance_matrix_greedy = q_learning_train(env,alpha=0.3,gamma=0.85)\n",
        "\n",
        "title1 = f\"Q-learning performance using epsilon-greedy approach. Training with {iterations-1} iterations.\"\n",
        "interval=100\n",
        "plot_performance(title1,q_learning_performance_matrix_greedy,xlabel=f\"Iterations (every {interval}th iteration)\",interval=interval)\n",
        "\n",
        "#############################################\n",
        "\n",
        "q_learning_greedy_epoch_count = []\n",
        "for i in range(1000):\n",
        "    q_learning_greedy_epoch_count.append(q_learning_performance_matrix_greedy[i][0])\n",
        "    \n",
        "sns.distplot(q_learning_greedy_epoch_count,color='mediumseagreen')\n",
        "plt.title(\"Distribution of number of steps needed\")\n",
        "\n",
        "\n",
        "print(\"An agent using Q Learning Greedy takes about an average of \" + str(int(np.mean(q_learning_greedy_epoch_count)))\n",
        "      + \" steps to successfully complete its mission.\")\n",
        "\n",
        "#as epsilon increases we explore more and exploit less and vice versa\n",
        "#epsilon = 0.1 means explore, epsilon=0.9 means exploit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLklBGmGHToN"
      },
      "source": [
        "iterations,q_learning_frame_exploit,q_table_exploit,q_learning_performance_matrix_exploit = q_learning_train(env,alpha=0.3,gamma=0.85,epsilon=0.9)\n",
        "\n",
        "title1 = f\"Q-learning performance using exploit-only approach. Training with {iterations-1} iterations.\"\n",
        "interval=100\n",
        "plot_performance(title1,q_learning_performance_matrix_exploit,xlabel=f\"Iterations (every {interval}th iteration)\",interval=interval)\n",
        "\n",
        "###################################################\n",
        "\n",
        "q_learning_exploit_epoch_count = []\n",
        "for i in range(1000):\n",
        "    q_learning_exploit_epoch_count.append(q_learning_performance_matrix_exploit[i][0])\n",
        "    \n",
        "sns.distplot(q_learning_exploit_epoch_count,color='mediumseagreen')\n",
        "plt.title(\"Distribution of number of steps needed\")\n",
        "\n",
        "\n",
        "print(\"An agent using Q Learning Exploit takes about an average of \" + str(int(np.mean(q_learning_exploit_epoch_count)))\n",
        "      + \" steps to successfully complete its mission.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYKqrpkFHXCT"
      },
      "source": [
        "iterations,q_learning_frame_explore,q_table_explore,q_learning_performance_matrix_explore = q_learning_train(env,alpha=0.3,gamma=0.85,epsilon=0.1)\n",
        "\n",
        "title1 = f\"Q-learning performance using explore-only approach. Training with {iterations-1} iterations.\"\n",
        "interval=100\n",
        "plot_performance(title1,q_learning_performance_matrix_explore,xlabel=f\"Iterations (every {interval}th iteration)\",interval=interval)\n",
        "\n",
        "###################################################\n",
        "\n",
        "q_learning_explore_epoch_count = []\n",
        "for i in range(1000):\n",
        "    q_learning_explore_epoch_count.append(q_learning_performance_matrix_explore[i][0])\n",
        "    \n",
        "sns.distplot(q_learning_explore_epoch_count,color='mediumseagreen')\n",
        "plt.title(\"Distribution of number of steps needed\")\n",
        "\n",
        "\n",
        "print(\"An agent using Q Learning Explore takes about an average of \" + str(int(np.mean(q_learning_explore_epoch_count)))\n",
        "      + \" steps to successfully complete its mission.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJLiJCZAHaTG"
      },
      "source": [
        "#performance_matrix_b,frames_brute_force =   brute_force(iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUGiXsYvHfzw"
      },
      "source": [
        "\n",
        "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
        "\n",
        "    def _show_on_single_plot(ax):\n",
        "      \n",
        "        if h_v == \"v\":\n",
        "            for p in ax.patches:\n",
        "                _x = p.get_x() + p.get_width() / 2\n",
        "                _y = p.get_y() + p.get_height()\n",
        "                value = int(p.get_height())\n",
        "                ax.text(_x, _y, value, ha=\"center\") \n",
        "        elif h_v == \"h\":\n",
        "            for p in ax.patches:\n",
        "                _x = p.get_x() + p.get_width() + float(space)\n",
        "                _y = p.get_y() + p.get_height()/2\n",
        "                value = int(p.get_width())\n",
        "                ax.text(_x, _y, value ,va=\"center\", ha=\"left\")\n",
        "\n",
        "    if isinstance(axs, np.ndarray):\n",
        "        for idx, ax in np.ndenumerate(axs):\n",
        "            _show_on_single_plot(ax)\n",
        "    else:\n",
        "        _show_on_single_plot(axs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMT8ETV0XoU3"
      },
      "source": [
        "\n",
        "# ----------------------------   Deep Q Network   ----------------------------------\n",
        "\n",
        "\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Uv1OuqP_qL9AJOf8wqK9bu72aaeWI_kC\" width=\"600\" height=\"230\">\n",
        "\n",
        "**Packages**\n",
        "\n",
        "\n",
        "First, let's import needed packages. Firstly, we need\n",
        "`gym <https://gym.openai.com/docs>`__ for the environment\n",
        "(Install using `pip install gym`).\n",
        "We'll also use the following from PyTorch:\n",
        "\n",
        "-  neural networks (``torch.nn``)\n",
        "-  optimization (``torch.optim``)\n",
        "-  automatic differentiation (``torch.autograd``)\n",
        "-  utilities for vision tasks (``torchvision`` - `a separate\n",
        "   package <https://github.com/pytorch/vision>`__).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J__fvMhK1UmR"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "!pip install gym\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voghm9yK1VhC"
      },
      "source": [
        "# for using torch lib\n",
        "\n",
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uh72zpKdnNZ"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import torch_xla\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from __future__ import division\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "from torch.autograd import Variable\n",
        "import torch_xla.core.xla_model as xm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xDq6MgY1ZV2"
      },
      "source": [
        "env = gym.envs.make(\"Taxi-v3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPIjwF-41ZYw"
      },
      "source": [
        "dev = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAPOuwZ_1ZcB"
      },
      "source": [
        "def plot_res(values, title=''):   \n",
        "\n",
        "    ''' Plot the reward curve and histogram of results over time '''\n",
        "    \n",
        "    # Update the window after each episode\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    # Define the figure\n",
        "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
        "    f.suptitle(title)\n",
        "    ax[0].plot(values, label='score per run')\n",
        "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
        "    ax[0].set_xlabel('Episodes')\n",
        "    ax[0].set_ylabel('Reward')\n",
        "    x = range(len(values))\n",
        "    ax[0].legend()\n",
        "\n",
        "\n",
        "    # Calculate the trend\n",
        "    try:\n",
        "        z = np.polyfit(x, values, 1)\n",
        "        p = np.poly1d(z)\n",
        "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
        "    except:\n",
        "        print('')\n",
        "    \n",
        "    # Plot the histogram of results\n",
        "    ax[1].hist(values[-50:])\n",
        "    ax[1].axvline(195, c='red', label='goal')\n",
        "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
        "    ax[1].set_ylabel('Frequency')\n",
        "    ax[1].legend()\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7M1NFyp1Zgx"
      },
      "source": [
        "def plot_performance(title,performance_matrix,**kwargs):\n",
        "\n",
        "\n",
        "    epochs=[]\n",
        "    penalties=[]\n",
        "    explore=[]\n",
        "    exploit=[]\n",
        "    \n",
        "    for i,episode_performance in enumerate(performance_matrix):\n",
        "        if 'interval' in kwargs:\n",
        "            if i%kwargs['interval']==0:\n",
        "                epochs.append(episode_performance[0])\n",
        "                penalties.append(episode_performance[1])\n",
        "        else:\n",
        "            epochs.append(episode_performance[0])\n",
        "            penalties.append(episode_performance[1])\n",
        "    \n",
        "    df=pd.DataFrame({'episodes': range(0,len(epochs)), 'epochs': epochs, 'penalties': penalties })\n",
        "    plt.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "    #plt.ylim(top=toplimit)\n",
        "\n",
        "    if 'ylimit' in kwargs:\n",
        "        plt.ylim(kwargs['ylimit'])\n",
        "    if 'xlimit' in kwargs:\n",
        "        plt.xlim(kwargs['xlimit'])\n",
        "    if 'xlabel' in kwargs:\n",
        "        plt.xlabel(kwargs['xlabel'])\n",
        "    if 'ylabel' in kwargs:\n",
        "        plt.ylabel(kwargs['ylabel'])\n",
        "\n",
        "    plt.plot( 'episodes', 'epochs', data=df, marker='o', markerfacecolor='navy', markersize=1, color='mediumblue', linewidth=4)\n",
        "    plt.plot( 'episodes', 'penalties', data=df, marker='o', markerfacecolor='red', markersize=1, color='darkred', linewidth=4)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    if 'save' in kwargs:\n",
        "        plt.savefig(f\"{kwargs['save']}\")\n",
        "\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_3g44kN1Zi_"
      },
      "source": [
        "class DQL():\n",
        "  \n",
        "    ''' Deep Q Neural Network class '''\n",
        "\n",
        "    \n",
        "    __constants__ = ['state_dim', 'action_dim', 'hidden_dim', 'lr', 'dropout']\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, device=None, hidden_dim=64, lr=0.05, dropout=0.4):\n",
        "\n",
        "            #super(Linear, self).__init__()\n",
        "            self.state_dim = state_dim\n",
        "            self.action_dim = action_dim\n",
        "            self.hidden_dim = hidden_dim\n",
        "            self.lr = lr\n",
        "            self.dropout = dropout\n",
        "            self.criterion = torch.nn.MSELoss()\n",
        "\n",
        "            self.model = torch.nn.Sequential(\n",
        "                            torch.nn.Linear(state_dim, hidden_dim),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Dropout(dropout),                        \n",
        "                            torch.nn.Linear(hidden_dim, action_dim)\n",
        "                    )\n",
        "            if device is not None:\n",
        "              self.device=device\n",
        "              self.model = self.model.to(device)\n",
        "              \n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "            self.num_param_updates=0\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, state, y):\n",
        "      \n",
        "        \"\"\" Update the weights of the network given a training sample \"\"\"\n",
        "\n",
        "        self.model.train()\n",
        "        state = torch.Tensor(state).to(self.device)\n",
        "        y = Variable(torch.Tensor(y)).to(self.device)\n",
        "        y_pred = self.model(state)\n",
        "        loss = self.criterion(y_pred, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(self.optimizer, barrier=True)\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, state):\n",
        "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            return self.model(torch.Tensor(state).to(self.device))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFU5zKsY1ZoD"
      },
      "source": [
        "''' the algorithm is modified to take as input a one-hot vector \n",
        "    epresenting one of 500 possible states of the taxi environment.'''\n",
        "\n",
        "class OneHotGenerator():\n",
        "\n",
        "    def __init__(self, num_labels):\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.one_hot_array = np.eye(num_labels)\n",
        "    \n",
        "    \n",
        "    def get_one_hot(self, label):\n",
        "\n",
        "        return self.one_hot_array[label]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfYtESm6dnQn"
      },
      "source": [
        "class ExponentialSchedule():\n",
        "  \n",
        "    #Exponential scheduling strategy.\n",
        "    \n",
        "    def __init__(self, start_val = 1.0, end_val = 0.05, decay_rate = 200):\n",
        "\n",
        "        self.start = start_val # initial value (float)\n",
        "        self.end = end_val # final value (float)\n",
        "        self.decay = decay_rate # rate of exponential decaying (int), usually steps or episodes\n",
        "\n",
        "\n",
        "    def value(self, t):\n",
        "        #Calculates the current value at time t\n",
        "\n",
        "        return (self.end + (self.start - self.end) * np.exp(-1.0 * t / self.decay))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VGW7HiN2Swp"
      },
      "source": [
        "def learn(model_target, model_train, device, memory, size, n_update, gamma=0.9):\n",
        "  \n",
        "        \"\"\" Add experience replay to the DQN network class \"\"\"\n",
        "        \n",
        "        # Make sure the memory is big enough\n",
        "        if len(memory) >= size:\n",
        "            states = []\n",
        "            targets = []\n",
        "\n",
        "            # Sample a batch of experiences from the agent's memory\n",
        "            batch = random.sample(memory, size)\n",
        "            # print(batch)\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            batch = np.array(batch)\n",
        "            # print(batch)\n",
        "\n",
        "            state_batch = torch.from_numpy(np.stack(batch[:, 0])).type(torch.FloatTensor).to(device)\n",
        "            # print(state_batch.size())\n",
        "\n",
        "            action_batch = torch.from_numpy(np.stack(batch[:, 1])).long().to(device)\n",
        "            # print(action_batch.size())\n",
        "\n",
        "            next_batch = torch.from_numpy(np.stack(batch[:, 2])).type(torch.FloatTensor).to(device)\n",
        "            # print(next_batch.size())\n",
        "\n",
        "            rew_batch = torch.from_numpy(np.stack(batch[:, 3])).to(device)\n",
        "            # print(rew_batch.size())\n",
        "            \n",
        "            not_done_mask = torch.from_numpy(1 - np.stack(batch[:, 4])).to(device)\n",
        "            # print(not_done_mask.size())\n",
        "\n",
        "            q_values = model_train.model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "\n",
        "            next_max_q = model_target.model(next_batch).detach().max(1)[0]\n",
        "            next_q_values = not_done_mask*next_max_q\n",
        "            target_q_values = rew_batch + (gamma*next_q_values)\n",
        "          \n",
        "\n",
        "            for_loop_duration = time.time() - start_time\n",
        "            # print(\"For loop took %.2f secs\" % for_loop_duration)\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            loss = model_train.criterion(q_values, target_q_values.unsqueeze(1))\n",
        "            model_train.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(model_train.optimizer, barrier=True)\n",
        "\n",
        "\n",
        "            model_update_time = time.time() - start_time\n",
        "            # print(\"Model update took %.2f secs\" % model_update_time)\n",
        "\n",
        "            model_train.num_param_updates += 1\n",
        "            if model_train.num_param_updates % n_update ==0:\n",
        "                model_target.model.load_state_dict(model_train.model.state_dict())\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-qv3fLx2S1q"
      },
      "source": [
        "def q_learning(env, model_train, model_target, device, episodes, gamma=0.85, \n",
        "               epsilon=0.3, eps_decay=0.99, replay=False, replay_size=150, \n",
        "               title = 'DQL', double=False, n_update=1000, soft=False):\n",
        "  \n",
        "    \"\"\" Deep Q Learning algorithm using the DQN \"\"\"\n",
        "\n",
        "    final = []\n",
        "    memory = []\n",
        "    one_hot_gen =  OneHotGenerator(env.observation_space.n)\n",
        "    scheduler = ExponentialSchedule()\n",
        "    epsilon = scheduler.value(0)\n",
        "    q_learning_performance_matrix = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        if double and not soft:\n",
        "\n",
        "            # Update target network every n_update steps\n",
        "            if episode % n_update == 0:\n",
        "                model.target_update()\n",
        "        if double and soft:\n",
        "            model.target_update()\n",
        "        \n",
        "        # Reset state\n",
        "        state = env.reset()\n",
        "        state = one_hot_gen.get_one_hot(state)\n",
        "\n",
        "        done = False\n",
        "        total = 0\n",
        "        steps = 0\n",
        "        penalties = 0\n",
        "        \n",
        "        epsilon = scheduler.value(episode+1)\n",
        "\n",
        "        while not done:\n",
        "            # Implement greedy search policy to explore the state space\n",
        "            steps +=1            \n",
        "\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_values = model_target.predict(state)\n",
        "                action = torch.argmax(q_values).item()\n",
        "            \n",
        "            # Take action and add reward to total\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = one_hot_gen.get_one_hot(next_state)\n",
        "\n",
        "            if reward==-10:\n",
        "                penalties+=1\n",
        "            total+=penalties\n",
        "            memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "            if steps>100:\n",
        "                learn(model_target, model_train, device, memory, replay_size, n_update, gamma)\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # Update epsilon\n",
        "        q_learning_performance_matrix.append([steps, penalties])\n",
        "        final.append(total)\n",
        "        plot_res(final, title)\n",
        "\n",
        "    return final, q_learning_performance_matrix\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDBAMlc62S4s"
      },
      "source": [
        "# Num of states\n",
        "n_state = env.observation_space.n\n",
        "# Num of actions\n",
        "n_action = env.action_space.n\n",
        "# Num of episodes\n",
        "episodes = 150\n",
        "# Num of hidden nodes in the DQN\n",
        "n_hidden = 150\n",
        "# Learning rate\n",
        "lr = 0.0003"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb0R5u2IQv4q"
      },
      "source": [
        "#Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsBTAnXY2hUL"
      },
      "source": [
        "dqn_train = DQL(n_state, 6, dev, n_hidden, lr)\n",
        "dqn_target= DQL(n_state, 6, dev, n_hidden, lr)\n",
        "\n",
        "dqn_train.model.train()\n",
        "dqn_target.model.eval()\n",
        "\n",
        "replay = q_learning(env, dqn_train, dqn_target, dev,\n",
        "                    1000, gamma=0.85, \n",
        "                    epsilon=0.3, replay=True, \n",
        "                    title='DQL with Replay')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHbk9lFy2haE"
      },
      "source": [
        "dqn_train = DQL(n_state, 6, dev, n_hidden, lr)\n",
        "dqn_target= DQL(n_state, 6, dev, n_hidden, lr)\n",
        "\n",
        "dqn_train.model.train()\n",
        "dqn_target.model.eval()\n",
        "\n",
        "final, q_learning_performance_matrix = q_learning(env, dqn_train, dqn_target, dev,\n",
        "                    1000, gamma=0.85, \n",
        "                    epsilon=0.3, replay=True, \n",
        "                    title='DQL with Replay')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZpjqIjldnX2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "title1 = f\"Deep Q-learning performance with 1000 episodes.\"\n",
        "interval=100\n",
        "plot_performance(title1,q_learning_performance_matrix,xlabel=f\"Iterations (every {interval}th iteration)\")\n",
        "\n",
        "\n",
        "d_q_learning_epoch_count = []\n",
        "for i in range(1000):\n",
        "    d_q_learning_epoch_count.append(q_learning_performance_matrix[i][0])\n",
        "    \n",
        "sns.distplot(d_q_learning_epoch_count,color='mediumseagreen')\n",
        "plt.title(\"Distribution of number of steps needed\")\n",
        "\n",
        "\n",
        "print(\"An agent using Deep Q Learning Explore takes about an average of \" + str(int(np.mean(d_q_learning_epoch_count)))\n",
        "      + \" steps to successfully complete its mission.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4T8GyHIdnft"
      },
      "source": [
        "np.array(q_learning_performance_matrix)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vEMC5y3LsKy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF5bAnrFtCzs"
      },
      "source": [
        "# Comparing Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHeSHuoBKdVB"
      },
      "source": [
        "# Table comparing\n",
        "data = [get_performance_df(q_learning_performance_matrix_greedy), get_performance_df(q_learning_performance_matrix_exploit),\n",
        "        get_performance_df(q_learning_performance_matrix_explore), get_performance_df(q_learning_performance_matrix)]\n",
        "df = pd.DataFrame(data,index=['Q-Learning-Greedy','Q-Learning-Exploit','Q-Learning-Explore','Deep Q-Learning'],columns=['Average epochs','Average penalties'])\n",
        "print(f\"Average for {1000} episodes/iterations\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xHjgi3jMdwm"
      },
      "source": [
        "# comparing in term of average steps needed in each algorithms  \n",
        "\n",
        "list1 = ['Q-Learning-Greedy','Q-Learning-Exploit','Q-Learning-Explore','Deep Q-Learning']\n",
        "list2 = []\n",
        "list2.append(int(np.mean(q_learning_greedy_epoch_count)))\n",
        "list2.append(int(np.mean(q_learning_exploit_epoch_count)))\n",
        "list2.append(int(np.mean(q_learning_explore_epoch_count)))\n",
        "list2.append(int(np.mean(d_q_learning_epoch_count)))\n",
        "\n",
        "\n",
        "df = pd.DataFrame(list(zip(list1, list2)), \n",
        "               columns =['Algorithms', 'Average Steps Required']) \n",
        "df\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(y=\"Algorithms\", x=\"Average Steps Required\", data=df)\n",
        "show_values_on_bars(ax, \"h\", 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d2BQUBL2y4"
      },
      "source": [
        "# plot sns: distribution  of  number  of  steps needed in different algorithms\n",
        "\n",
        "# sns.set(font_scale=1)\n",
        "plt.figure(figsize=(17, 10))\n",
        "sns.distplot(q_learning_greedy_epoch_count, hist=False, rug=False, label=\"Q_learning_greedy\")\n",
        "sns.distplot(q_learning_exploit_epoch_count, hist=False, rug=False, label=\"Q_learning_exploit\")\n",
        "sns.distplot(q_learning_explore_epoch_count, hist=False, rug=False, label=\"Q_learning_explore\")\n",
        "sns.distplot(d_q_learning_epoch_count, hist=False, rug=False, label=\"Deep Q_learning\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
